{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bc86ee4-8755-4891-b639-9c9fc212075a",
   "metadata": {},
   "source": [
    "# Lexical Analyzer for Simple Arithmetic Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614212ae-2454-4ec8-a67e-89261ba6f476",
   "metadata": {},
   "source": [
    "### Author : Vinuka Vinnath\n",
    "### Index No : D/BCS/22/0005"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd96d5e7-c7e1-484c-b30f-168c5f1d82c8",
   "metadata": {},
   "source": [
    "### Overview\n",
    "This program implements a lexical analyzer for a simple arithmetic expression language. It tokenizes an input string by breaking it down into meaningful units called tokens. The lexical analyzer uses the state machine concept, where the program transitions between different states based on the current character being processed. This approach ensures that each token is correctly identified and the input string is parsed in a structured manner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004bd365-0473-42b2-9f61-0bc621938469",
   "metadata": {},
   "source": [
    "### States\n",
    "The States class defines the possible states of the lexical analyzer.\n",
    "\n",
    "- `START`: The initial state, where the lexer checks each character to determine whether it's part of an identifier, number, or special symbol.\n",
    "- `READING_IDENTIFIER`: A state used when the lexer is reading a sequence of alphanumeric characters (letters and digits) that form an identifier (e.g., x, variable1).\n",
    "- `READING_NUMBER`: A state used when the lexer is reading a sequence of digits to form a number (e.g., 10, 42).\n",
    "- `DONE`: A final state indicating that the lexical analysis is complete (not used directly in this version, but can be used for future state management)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1aaac1b-9f10-4d72-945a-f0ee9321f45f",
   "metadata": {},
   "source": [
    "### Token Types\n",
    "The following token types are recognized by the lexical analyzer:\n",
    "- `IDENTIFIER`: Variable names, consisting of letters and digits (e.g., x, y, variable1).\n",
    "- `NUMBER`: Numeric constants, consisting of digits (e.g., 10, 42).\n",
    "- `ASSIGNMENT`: The assignment operator =.\n",
    "- `OPERATOR`: Arithmetic operators (+, -, *, /).\n",
    "- `PARENTHESIS`: Parentheses ( and ).\n",
    "- `SEMICOLON`: The semicolon ;."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cbbfcc-80ed-455e-8577-bc88054f4325",
   "metadata": {},
   "source": [
    "### Main Functionality : `lexical_analyzer(input_string)`\n",
    "This function tokenizes an input string into a list of tokens, based on the current character and the state of the lexer. It processes the string character by character and handles transitions between different states.\n",
    "- Parameters: `input_string (str)`: The string containing the arithmetic expression that needs to be tokenized.\n",
    "- Returns: A list of tokens. Each token is represented as a tuple of two elements consist of Token Type and Token Value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5319ec0b-2d19-4f3f-bbd7-9c5ee659c281",
   "metadata": {},
   "source": [
    "## Source Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "bc284341-e0dd-4e6d-8169-ebdf9f0607b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the possible states in the lexical analyzer\n",
    "class States:\n",
    "    START = \"START\"\n",
    "    READING_IDENTIFIER = \"READING_IDENTIFIER\"  \n",
    "    READING_NUMBER = \"READING_NUMBER\" \n",
    "    DONE = \"DONE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3141fb0d-9c91-4c6c-b9b0-2af56b1d6795",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_analyzer(input_string):\n",
    "    \n",
    "    \"\"\"\n",
    "    Tokenizes the input string into a list of tokens using a state machine approach.\n",
    "    \"\"\"\n",
    "    \n",
    "    tokens = []           # List to store the generated tokens\n",
    "    state = States.START  # Initial state of the lexer\n",
    "    current_token = \"\"    # Variable to store the current token being formed\n",
    "    index = 0             # Pointer to traverse the input string\n",
    "\n",
    "    while index < len(input_string):\n",
    "        char = input_string[index]  # Get the current character from the input string\n",
    "\n",
    "        # State handling: Starting state where the lexer checks character type\n",
    "        if state == States.START:\n",
    "            \n",
    "            if char.isspace():\n",
    "                # If the character is a space, just skip it\n",
    "                index += 1\n",
    "                \n",
    "            elif char.isalpha():\n",
    "                # If it's a letter (part of an identifier), transition to READING_IDENTIFIER state\n",
    "                state = States.READING_IDENTIFIER\n",
    "                current_token = char  # Start building the identifier\n",
    "                index += 1\n",
    "                \n",
    "            elif char.isdigit():\n",
    "                # If it's a digit, transition to READING_NUMBER state\n",
    "                state = States.READING_NUMBER\n",
    "                current_token = char  # Start building the number\n",
    "                index += 1\n",
    "                \n",
    "            elif char == '=':\n",
    "                # If it's an assignment operator '=', generate the ASSIGNMENT token\n",
    "                tokens.append(('ASSIGNMENT', char))\n",
    "                index += 1\n",
    "                \n",
    "            elif char in '+-*/':\n",
    "                # If it's an arithmetic operator (+, -, *, /), generate the OPERATOR token\n",
    "                tokens.append(('OPERATOR', char))\n",
    "                index += 1\n",
    "                \n",
    "            elif char in '()':\n",
    "                # If it's a parenthesis ('(', ')'), generate the PARENTHESIS token\n",
    "                tokens.append(('PARENTHESIS', char))\n",
    "                index += 1\n",
    "                \n",
    "            elif char == ';':\n",
    "                # If it's a semicolon, generate the SEMICOLON token\n",
    "                tokens.append(('SEMICOLON', char))\n",
    "                index += 1\n",
    "                \n",
    "            else:\n",
    "                # If the character is unrecognized, raise an error\n",
    "                raise ValueError(f\"Unrecognized character: {char}\")\n",
    "                \n",
    "\n",
    "        # State handling: Reading an identifier (variable name)\n",
    "        elif state == States.READING_IDENTIFIER:\n",
    "            if char.isalnum():\n",
    "                # If the character is alphanumeric, continue building the identifier\n",
    "                current_token += char\n",
    "                index += 1\n",
    "                \n",
    "            else:\n",
    "                # If a non-alphanumeric character is encountered, emit the identifier token\n",
    "                tokens.append(('IDENTIFIER', current_token))\n",
    "                current_token = \"\"  # Reset current token\n",
    "                state = States.START  # Return to the START state to process the next character\n",
    "                \n",
    "\n",
    "        # State handling: Reading a number (numeric value)\n",
    "        elif state == States.READING_NUMBER:\n",
    "            if char.isdigit():\n",
    "                # If the character is a digit, continue building the number\n",
    "                current_token += char\n",
    "                index += 1\n",
    "                \n",
    "            else:\n",
    "                # If a non-digit character is encountered, emit the number token\n",
    "                tokens.append(('NUMBER', current_token))\n",
    "                current_token = \"\"  # Reset current token\n",
    "                state = States.START  # Return to the START state to process the next character\n",
    "                \n",
    "\n",
    "    # Handle the case where the input ends with an unprocessed identifier or number\n",
    "    if state == States.READING_IDENTIFIER:\n",
    "        tokens.append(('IDENTIFIER', current_token))\n",
    "    elif state == States.READING_NUMBER:\n",
    "        tokens.append(('NUMBER', current_token))\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc00bf5-9ae7-4ac1-b7a0-69058b561b45",
   "metadata": {},
   "source": [
    "## Test Cases\n",
    "Below code block serves as a script to verify the correctness of the lexical analyzer function by testing it on predefined input expressions.\n",
    "\n",
    "Tested expressions are as follows:\n",
    "- `x = 10 + 20 * (30 / y);`\n",
    "- `a = b + 5;`\n",
    "- `(x * y) + z;`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b0932f57-a1b5-40ff-82e5-4936c4e23933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexical Analyzer Results:\n",
      "\n",
      "Input: x = 10 + 20 * (30 / y);\n",
      "('IDENTIFIER', 'x')\n",
      "('ASSIGNMENT', '=')\n",
      "('NUMBER', '10')\n",
      "('OPERATOR', '+')\n",
      "('NUMBER', '20')\n",
      "('OPERATOR', '*')\n",
      "('PARENTHESIS', '(')\n",
      "('NUMBER', '30')\n",
      "('OPERATOR', '/')\n",
      "('IDENTIFIER', 'y')\n",
      "('PARENTHESIS', ')')\n",
      "('SEMICOLON', ';')\n",
      "\n",
      "Input: a = b + 5;\n",
      "('IDENTIFIER', 'a')\n",
      "('ASSIGNMENT', '=')\n",
      "('IDENTIFIER', 'b')\n",
      "('OPERATOR', '+')\n",
      "('NUMBER', '5')\n",
      "('SEMICOLON', ';')\n",
      "\n",
      "Input: (x * y) + z;\n",
      "('PARENTHESIS', '(')\n",
      "('IDENTIFIER', 'x')\n",
      "('OPERATOR', '*')\n",
      "('IDENTIFIER', 'y')\n",
      "('PARENTHESIS', ')')\n",
      "('OPERATOR', '+')\n",
      "('IDENTIFIER', 'z')\n",
      "('SEMICOLON', ';')\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    test_expressions = [\n",
    "        \"x = 10 + 20 * (30 / y);\",\n",
    "        \"a = b + 5;\",\n",
    "        \"(x * y) + z;\",\n",
    "    ]\n",
    "\n",
    "    print(\"Lexical Analyzer Results:\")\n",
    "    for expr in test_expressions:\n",
    "        print(f\"\\nInput: {expr}\")\n",
    "        try:\n",
    "            tokens = lexical_analyzer(expr)\n",
    "            for token in tokens:\n",
    "                print(token)\n",
    "        except ValueError as e:\n",
    "            print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66574f4b-a08f-4f46-8dc3-bae46e390073",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
